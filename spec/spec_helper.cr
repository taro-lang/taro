require "spec"
require "./token_helper"
require "../src/all"
require "./support/nodes.cr"

include Taro::Compiler
include Taro::Compiler::Ast
include TokenHelper

# Run the Taro parser on the given source code, returning the AST that the
# parser generates for it.
def parse_program(source : String) : Expressions
  parser = Parser.new(IO::Memory.new(source), File.join(Dir.current, "test_source.taro"))
  parser.parse
end

def it_parses(source, *expected)
  result = parse_program(source)
  unless expected.empty?
    result.should eq(Expressions.new(*expected))
  end
end

def it_does_not_parse(source, message = nil)
  exception = expect_raises(ParseError) do
    result = parse_program(source)
  end

  if message
    (exception.message || "").downcase.should match(message)
  end
end

# Return the list of tokens generated by lexing the given source. Parsing is
# not performed, so semantically-invalid sequences are allowed by this method.
def tokenize(source : String, in_context : Lexer::Context? = nil)
  lexer = Lexer.new(IO::Memory.new(source), File.join(Dir.current, "test_source.taro"))
  if in_context
    lexer.push_context(in_context)
  end
  lexer.lex_all
  lexer.tokens
end
